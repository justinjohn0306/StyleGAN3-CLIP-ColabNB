{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StyleGAN3+CLIP.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJOj_BWi_JwR"
      },
      "source": [
        "# **StyleGAN3 + CLIP 🖼️**\n",
        "\n",
        "## Generate images (mostly faces) from text prompts using NVIDIA's StyleGAN3 with CLIP guidance.\n",
        "\n",
        "Code written by [nshepperd](https://twitter.com/nshepperd1) (https://github.com/nshepperd).\n",
        "\n",
        "Modified by [justinjohn0306](https://github.com/justinjohn0306)\n",
        "\n",
        "Thanks to [Katherine Crowson](https://twitter.com/RiversHaveWings) (https://github.com/crowsonkb) for coming up with many improved sampling tricks, as well as some of the code.\n",
        "\n",
        "\n",
        "**Visit StyleGAN3**, [here](https://github.com/NVlabs/stylegan3)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "so1yHofG7RxX"
      },
      "source": [
        "#@title Licensed under the MIT License { display-mode: \"form\" }\n",
        "\n",
        "# Copyright (c) 2021 nshepperd; Katherine Crowson\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
        "# THE SOFTWARE."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rg_x6IdHv_2G",
        "cellView": "form"
      },
      "source": [
        "#@markdown #**Check GPU** 🕵️\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "!nvidia-smi -L\n",
        "!nvcc --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K38uyFrv5wo",
        "cellView": "form"
      },
      "source": [
        "#@markdown #**Install libraries** 🏗️\n",
        "# @markdown This cell will take a little while because it has to download several libraries.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "!pip install --upgrade torch==1.9.1+cu111 torchvision==0.10.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "#!pip install --upgrade https://download.pytorch.org/whl/nightly/cu111/torch-1.11.0.dev20211012%2Bcu111-cp37-cp37m-linux_x86_64.whl https://download.pytorch.org/whl/nightly/cu111/torchvision-0.12.0.dev20211012%2Bcu111-cp37-cp37m-linux_x86_64.whl\n",
        "!git clone https://github.com/NVlabs/stylegan3\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!pip install -e ./CLIP\n",
        "!pip install einops ninja\n",
        "\n",
        "import sys\n",
        "sys.path.append('./CLIP')\n",
        "sys.path.append('./stylegan3')\n",
        "\n",
        "import io\n",
        "import os, time\n",
        "import pickle\n",
        "import shutil\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import requests\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import clip\n",
        "from tqdm.notebook import tqdm\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
        "from IPython.display import display\n",
        "from einops import rearrange\n",
        "from google.colab import files\n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "print('Using device:', device, file=sys.stderr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVkNODOot_To",
        "cellView": "form"
      },
      "source": [
        "#@markdown #**Optional:** Save images in Google Drive 💾\n",
        "# @markdown Run this cell if you want to store the results inside Google Drive.\n",
        "\n",
        "# @markdown Copying the generated images to drive is faster to work with.\n",
        "\n",
        "# @markdown **Important**: you must have a folder named *samples* inside your drive, otherwise this may not work.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "# Uncomment to copy generated images to drive, faster than downloading directly from colab in my experience.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GOWJ_z-wgde",
        "cellView": "form"
      },
      "source": [
        "#@markdown #**Define necessary functions** 🛠️\n",
        "\n",
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')\n",
        "\n",
        "def fetch_model(url_or_path):\n",
        "    basename = os.path.basename(url_or_path)\n",
        "    if os.path.exists(basename):\n",
        "        return basename\n",
        "    else:\n",
        "        !wget -c '{url_or_path}'\n",
        "        return basename\n",
        "\n",
        "def norm1(prompt):\n",
        "    \"Normalize to the unit sphere.\"\n",
        "    return prompt / prompt.square().sum(dim=-1,keepdim=True).sqrt()\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
        "\n",
        "class MakeCutouts(torch.nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        for _ in range(self.cutn):\n",
        "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
        "        return torch.cat(cutouts)\n",
        "\n",
        "make_cutouts = MakeCutouts(224, 32, 0.5)\n",
        "\n",
        "def embed_image(image):\n",
        "  n = image.shape[0]\n",
        "  cutouts = make_cutouts(image)\n",
        "  embeds = clip_model.embed_cutout(cutouts)\n",
        "  embeds = rearrange(embeds, '(cc n) c -> cc n c', n=n)\n",
        "  return embeds\n",
        "\n",
        "def embed_url(url):\n",
        "  image = Image.open(fetch(url)).convert('RGB')\n",
        "  return embed_image(TF.to_tensor(image).to(device).unsqueeze(0)).mean(0).squeeze(0)\n",
        "\n",
        "class CLIP(object):\n",
        "  def __init__(self):\n",
        "    clip_model = \"ViT-B/32\"\n",
        "    self.model, _ = clip.load(clip_model)\n",
        "    self.model = self.model.requires_grad_(False)\n",
        "    self.normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                          std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def embed_text(self, prompt):\n",
        "      \"Normalized clip text embedding.\"\n",
        "      return norm1(self.model.encode_text(clip.tokenize(prompt).to(device)).float())\n",
        "\n",
        "  def embed_cutout(self, image):\n",
        "      \"Normalized clip image embedding.\"\n",
        "      return norm1(self.model.encode_image(self.normalize(image)))\n",
        "  \n",
        "clip_model = CLIP()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VXU9WBvHbAX"
      },
      "source": [
        "Available models:\n",
        "*   **metfaces** for painings\n",
        "*   **afhqv2** for animals\n",
        "*   **ffhq** for photo faces\n",
        "*   **Cosplay Faces** trained by [@l4rz](https://twitter.com/l4rz)\n",
        "\n",
        "\n",
        "modes:\n",
        "*   **t** for translation\n",
        "*   **r** for rotation\n",
        "\n",
        "e.g.: *stylegan3-t-metfaces-1024x1024.pkl for painings with translation in 1024x1024 resolution*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vpIq2vvzTtS",
        "cellView": "form"
      },
      "source": [
        "#@markdown #**Model selection** 🎭\n",
        "\n",
        "#@markdown By default, the notebook downloads the FFHQ model.\n",
        "#@markdown **Run this cell again if you change the model**.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "base_url = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/\"\n",
        "\n",
        "#@markdown Choose a model:\n",
        "model_name = \"stylegan3-r-metfacesu-1024x1024.pkl\" #@param [\"stylegan2-cosplay-faces-512x512-px\", \"stylegan3-r-afhqv2-512x512.pkl\", \"stylegan3-r-ffhq-1024x1024.pkl\", \"stylegan3-r-ffhqu-1024x1024.pkl\",\"stylegan3-r-ffhqu-256x256.pkl\",\"stylegan3-r-metfaces-1024x1024.pkl\",\"stylegan3-r-metfacesu-1024x1024.pkl\",\"stylegan3-t-afhqv2-512x512.pkl\",\"stylegan3-t-ffhq-1024x1024.pkl\",\"stylegan3-t-ffhqu-1024x1024.pkl\",\"stylegan3-t-ffhqu-256x256.pkl\",\"stylegan3-t-metfaces-1024x1024.pkl\",\"stylegan3-t-metfacesu-1024x1024.pkl\"]\n",
        "network_url = base_url + model_name\n",
        "\n",
        "if model_name == \"stylegan2-cosplay-faces-512x512-px\":\n",
        "    network_url = 'https://l4rz.net/cosplayface-snapshot-004000-18160-FID367.pkl'\n",
        "\n",
        "with open(fetch_model(network_url), 'rb') as fp:\n",
        "  G = pickle.load(fp)['G_ema'].to(device)\n",
        "\n",
        "clip_model = CLIP()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_rq-N2m0Tlb",
        "cellView": "form"
      },
      "source": [
        "#@markdown #**Parameters** ✍️\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown **Enter your text prompt here:**\n",
        "text = 'Lady Dimitrescu with fangs ' #@param {type:\"string\"}\n",
        "target = clip_model.embed_text(text) \n",
        "\n",
        "#@markdown How many steps should it be doing?\n",
        "steps =  200#@param \n",
        "\n",
        "#@markdown Choose random seed (-1 for completly random)\n",
        "seed =  -1#@param \n",
        "if seed == -1:\n",
        "    seed = np.random.randint(0,2**32 - 1)\n",
        "\n",
        "#@markdown How often do you want to see the results?\n",
        "show_every_n_steps = 20 #@param\n",
        "#@markdown ___\n",
        "\n",
        "#@markdown **EXPERIMENTAL/W.I.P**: Do you want to mix 2 models? (try FFHQ with MetFaces) \n",
        "mix = \"No\" #@param [\"Yes\", \"No\"]\n",
        "model_1_name = \"stylegan3-r-metfaces-1024x1024.pkl\" #@param [\"stylegan3-r-ffhq-1024x1024.pkl\", \"stylegan3-r-ffhqu-1024x1024.pkl\",\"stylegan3-r-metfaces-1024x1024.pkl\",\"stylegan3-r-metfacesu-1024x1024.pkl\",\"stylegan3-t-ffhq-1024x1024.pkl\",\"stylegan3-t-ffhqu-1024x1024.pkl\",\"stylegan3-t-metfaces-1024x1024.pkl\",\"stylegan3-t-metfacesu-1024x1024.pkl\"] \n",
        "model_2_name = \"stylegan3-r-ffhq-1024x1024.pkl\" #@param [\"stylegan3-r-ffhq-1024x1024.pkl\", \"stylegan3-r-ffhqu-1024x1024.pkl\",\"stylegan3-r-metfaces-1024x1024.pkl\",\"stylegan3-r-metfacesu-1024x1024.pkl\",\"stylegan3-t-ffhq-1024x1024.pkl\",\"stylegan3-t-ffhqu-1024x1024.pkl\",\"stylegan3-t-metfaces-1024x1024.pkl\",\"stylegan3-t-metfacesu-1024x1024.pkl\"] \n",
        "proportion  = 0.5 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "\n",
        "if mix == 'Yes':\n",
        "    with open(fetch_model(base_url + model_1_name), 'rb') as fp:\n",
        "        G1 = pickle.load(fp)['G_ema'].to(device)\n",
        "\n",
        "    #START_TEST\n",
        "    with open(fetch_model(base_url + model_2_name), 'rb') as fp:\n",
        "        G2 = pickle.load(fp)['G_ema'].to(device)\n",
        "\n",
        "    G = G2\n",
        "    for p_out, p_in1, p_in2 in zip(G.parameters(), G1.parameters(), G2.parameters()):\n",
        "        p_out.data = torch.nn.Parameter(p_in1*proportion+p_in2*(1-proportion));\n",
        "\n",
        "zs = torch.randn([10000, G.mapping.z_dim], device=device)\n",
        "w_stds = G.mapping(zs, None).std(0)\n",
        "#got it. I think its the w_std\n",
        "\n",
        "#@markdown Do you want to fix the coordinate grid? (False - wobbly effect)\n",
        "fix_coordinates = \"True\" #@param [\"True\", \"False\"]\n",
        "\n",
        "if fix_coordinates == \"True\":\n",
        "    if model_name != \"stylegan2-cosplay-faces-512x512-px\":\n",
        "        shift = G.synthesis.input.affine(G.mapping.w_avg.unsqueeze(0))\n",
        "        G.synthesis.input.affine.bias.data.add_(shift.squeeze(0))\n",
        "        G.synthesis.input.affine.weight.data.zero_()\n",
        "\n",
        "# target = embed_url(\"https://4.bp.blogspot.com/-uw859dFGsLc/Va5gt-bU9bI/AAAAAAAA4gM/dcaWzX0ZxdI/s1600/Lubjana+dragon+1.jpg\")\n",
        "# target = embed_url(\"https://irc.zlkj.in/uploads/e399d2fee2c6edd9/20210827165231_0_nexus%20of%20abandoned%20places.%20trending%20on%20ArtStation.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXoRP4SHzJ6i",
        "cellView": "form"
      },
      "source": [
        "#@markdown #**Run the model** 🚀\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "tf = Compose([\n",
        "  Resize(224),\n",
        "  lambda x: torch.clamp((x+1)/2,min=0,max=1),\n",
        "  ])\n",
        "\n",
        "def run(seed, G):\n",
        "  torch.manual_seed(seed)\n",
        "  timestring = time.strftime('%Y%m%d%H%M%S')\n",
        "  rand_z = torch.from_numpy(np.random.RandomState(seed).randn(1, G.mapping.z_dim)).to(device)\n",
        "  q = (G.mapping(rand_z, None, truncation_psi=0.2)) / w_stds\n",
        "  q.requires_grad_()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    qs = []\n",
        "    losses = []\n",
        "    for _ in range(8):\n",
        "      q = (G.mapping(torch.randn([4,G.mapping.z_dim], device=device), None, truncation_psi=0.7) - G.mapping.w_avg) / w_stds\n",
        "      images = G.synthesis(q * w_stds + G.mapping.w_avg)\n",
        "      embeds = embed_image(images.add(1).div(2))\n",
        "      loss = spherical_dist_loss(embeds, target).mean(0)\n",
        "      i = torch.argmin(loss)\n",
        "      qs.append(q[i])\n",
        "      losses.append(loss[i])\n",
        "    qs = torch.stack(qs)\n",
        "    losses = torch.stack(losses)\n",
        "    i = torch.argmin(losses)\n",
        "    q = qs[i].unsqueeze(0).requires_grad_()\n",
        "\n",
        "# Sampling loop\n",
        "  q_ema = q\n",
        "  opt = torch.optim.AdamW([q], lr=0.03, betas=(0.0,0.999))\n",
        "  loop = tqdm(range(steps))\n",
        "  for i in loop:\n",
        "    opt.zero_grad()\n",
        "    w = q * w_stds\n",
        "    image = G.synthesis(w + G.mapping.w_avg, noise_mode='const')\n",
        "    embed = embed_image(image.add(1).div(2))\n",
        "    loss = spherical_dist_loss(embed, target).mean()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    loop.set_postfix(loss=loss.item(), q_magnitude=q.std().item())\n",
        "\n",
        "    q_ema = q_ema * 0.9 + q * 0.1\n",
        "    image = G.synthesis(q_ema * w_stds + G.mapping.w_avg, noise_mode='const')\n",
        "\n",
        "    if i % 10 == 0:\n",
        "      display(TF.to_pil_image(tf(image)[0]))\n",
        "    pil_image = TF.to_pil_image(image[0].add(1).div(2).clamp(0,1))\n",
        "    os.makedirs(f'samples/{timestring}', exist_ok=True)\n",
        "    pil_image.save(f'samples/{timestring}/{i:04}.jpg')\n",
        "  \n",
        "  # Save images as a tar archive\n",
        "  !tar cf samples/{timestring}.tar samples/{timestring}\n",
        "\n",
        "  return timestring\n",
        "\n",
        "timestring = run(seed, G)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSnRMY8-_-iV",
        "cellView": "form"
      },
      "source": [
        "  #@markdown #**Save images** 📷\n",
        "  #@markdown A `.tar` file will be saved inside *samples* and automatically downloaded, unless you previously ran the Google Drive cell,\n",
        "  #@markdown in which case it'll be saved inside your previously created drive *samples* folder.\n",
        "  \n",
        "\n",
        "  # Save images as a tar archive\n",
        "  !tar cf samples/{timestring}.tar samples/{timestring}\n",
        "  if os.path.isdir('drive/MyDrive/samples'):\n",
        "    shutil.copyfile(f'samples/{timestring}.tar', f'drive/MyDrive/samples/{timestring}.tar')\n",
        "  else:\n",
        "    files.download(f'samples/{timestring}.tar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9Yyt8y99jfv",
        "cellView": "form"
      },
      "source": [
        "#@markdown #**Generate video** 🎥\n",
        "\n",
        "\n",
        "frames = os.listdir(f\"samples/{timestring}\")\n",
        "frames = len(list(filter(lambda filename: filename.endswith(\".jpg\"), frames))) #Get number of jpg generated\n",
        "\n",
        "init_frame = 1 #This is the frame where the video will start\n",
        "last_frame = frames #You can change i to the number of the last frame you want to generate. It will raise an error if that number of frames does not exist.\n",
        "\n",
        "min_fps = 10\n",
        "max_fps = 30\n",
        "\n",
        "total_frames = last_frame-init_frame\n",
        "\n",
        "#Desired video time in seconds\n",
        "video_length = 15 #@param {type:\"number\"}\n",
        "\n",
        "frames = []\n",
        "tqdm.write('Generating video...')\n",
        "for i in range(init_frame,last_frame): #\n",
        "    filename = f\"samples/{timestring}/{i:04}.jpg\"\n",
        "    frames.append(Image.open(filename))\n",
        "\n",
        "fps = np.clip(total_frames/video_length,min_fps,max_fps)\n",
        "fps = 30 #@param\n",
        "\n",
        "from subprocess import Popen, PIPE\n",
        "p = Popen(['ffmpeg', '-y', '-f', 'image2pipe', '-vcodec', 'png', '-r', str(fps), '-i', '-', '-vcodec', 'libx264', '-r', str(fps), '-pix_fmt', 'yuv420p', '-crf', '17', '-preset', 'veryslow', 'video.mp4'], stdin=PIPE)\n",
        "for im in tqdm(frames):\n",
        "    im.save(p.stdin, 'PNG')\n",
        "p.stdin.close()\n",
        "\n",
        "print(\"The video is now being compressed, wait...\")\n",
        "p.wait()\n",
        "print(\"The video is ready\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNpjDjR_-0dN",
        "cellView": "form"
      },
      "source": [
        "#@markdown #**Download video** 📀\n",
        "from google.colab import files\n",
        "files.download(\"video.mp4\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "LTvMW3ssK9Lh"
      },
      "source": [
        "#@markdown #**View video in browser** 👀\n",
        "\n",
        "# @markdown This process may take a little longer.\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('video.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5KAVLgCNTz7"
      },
      "source": [
        "JS to prevent idle timeout:\n",
        "\n",
        "Press F12 OR CTRL + SHIFT + I OR right click on this website -> inspect.\n",
        "Then click on the console tab and paste in the following code.\n",
        "\n",
        "```javascript\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "```"
      ]
    }
  ]
}